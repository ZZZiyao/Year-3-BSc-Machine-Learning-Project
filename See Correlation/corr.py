import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# è¯»å–æ•°æ®
data_path = r'D:\Year3\BSc Project\Particle-Machine-Learning\balanced_data.csv'
data = pd.read_csv(data_path, low_memory=False)

# è¯»å–ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ï¼Œé€‰æ‹©ç‰¹å¾
selected_features_path = r'D:\Year3\BSc Project\Particle-Machine-Learning\permutation_feature_importance.csv'
selected_features = pd.read_csv(selected_features_path)['Feature'].tolist()
data = data[selected_features]


# **åˆ é™¤ä½æ–¹å·®ç‰¹å¾ï¼ˆæ–¹å·®æ¥è¿‘ 0 çš„ç‰¹å¾ï¼‰**
low_variance_features = [
    "mu_minus_ProbNNd", "mu_plus_M", "pion_ProbNNd",
    "mu_plus_ProbNNd", "pion_M", "B0_ENDVERTEX_NDOF",
    "kaon_ProbNNd", "kaon_M", "mu_minus_M"
]
data_filtered = data.drop(columns=low_variance_features, errors="ignore")

# **è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ**
corr_matrix = data_filtered.corr().abs()

# **å–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆå»æ‰å¯¹è§’çº¿ï¼‰**
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# **æ‰¾åˆ°é«˜åº¦ç›¸å…³ï¼ˆ>0.9ï¼‰çš„ç‰¹å¾**
high_corr_features = [column for column in upper.columns if any(upper[column] > 0.9)]

print("ğŸ“Š é«˜åº¦ç›¸å…³ç‰¹å¾ï¼ˆ> 0.9ï¼‰ï¼š")
for feature in high_corr_features:
    max_corr = upper[feature].max()  # è·å–è¯¥ç‰¹å¾çš„æœ€é«˜ç›¸å…³æ€§
    related_feature = upper[feature].idxmax()  # è·å–ä¸å…¶æœ€é«˜ç›¸å…³çš„ç‰¹å¾
    print(f"âŒ {feature} (æœ€é«˜ç›¸å…³æ€§: {max_corr:.4f}ï¼Œä¸ {related_feature} ç›¸å…³)")

# **å¯é€‰ï¼šä¿å­˜ç›¸å…³æ€§çŸ©é˜µ**
corr_matrix.to_csv("feature_correlation_matrix.csv")
print("âœ… ç›¸å…³æ€§çŸ©é˜µå·²ä¿å­˜è‡³ feature_correlation_matrix.csv")
